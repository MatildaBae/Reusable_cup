# -*- coding: utf-8 -*-
"""data extraction_0918.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NPoliQa31w7t3fNK_MzlznfxM1QJizOZ
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install haversine

from haversine import haversine
import pandas as pd
import numpy as np

"""## 데이터 불러오기"""

# cafe = pd.read_csv('/content/drive/MyDrive/0.DAB/소상공인시장진흥공단_상가(상권)정보_서울_202403.csv')

datanew = pd.read_csv('/content/drive/MyDrive/0.DAB/fulldata_template0918.csv')
datanew.head()

datanew = datanew.iloc[:,[0,1,2,3,4]]

datanew.head()

datanew.iloc[1,]

datanew.info()

trashbin = pd.read_csv('/content/drive/MyDrive/0.DAB/trashbin.csv')
trashbin.head()

trashbin.info()

trashbin = trashbin.iloc[:,[0,1,2,3,4]]

trashbin.info()

trashbin.head()

"""## 데이터 전처리 - Cafe 데이터만 keep"""

cafe.info()

keep_column = [0, 1, 14, 16, 24, 31, 33, 37, 38]

# '상권업종소분류명'이 '카페'인 것들만 필터링
cafe_df = cafe[cafe['상권업종소분류명'] == '카페']

cafe_df = cafe_df.iloc[:, keep_column] # 최소한의 정보만 남기기
cafe_df.head()

"""## 근방 카페 개수 세기"""

from tqdm import tqdm

# 데이터 청크로 나누기 (예: 10개 청크로 나눔)
num_chunks = 6
chunks = np.array_split(datanew, num_chunks)

# 각 청크를 처리한 후, 해당 청크의 결과를 CSV 파일로 저장
def process_and_save_chunk(chunk, chunk_id, data, radius=0.3):
    result = []
    # tqdm을 사용하여 진행률 표시
    for idx, row in tqdm(chunk.iterrows(), total=chunk.shape[0], desc=f"Processing chunk {chunk_id}"):
        count = 0
        cafe_location = (row['Latitude'], row['Longitude'])
        for other_idx, other_row in data.iterrows():
            if idx != other_idx:
                other_location = (other_row['Latitude'], other_row['Longitude'])
                distance = haversine(cafe_location, other_location)
                if distance <= radius:
                    count += 1
        result.append(count)

    # 처리된 결과를 데이터프레임으로 변환하여 저장
    chunk['cafeinrange'] = result
    chunk_filename = f'chunk_{chunk_id}_result.csv'
    chunk.to_csv(chunk_filename, index=False)
    print(f"Chunk {chunk_id} saved to {chunk_filename}")
    return chunk_filename

# 각 청크를 처리하면서 바로 결과를 저장
saved_files = []
for i, chunk in enumerate(chunks):
    chunk_filename = process_and_save_chunk(chunk, i+1, datanew)
    saved_files.append(chunk_filename)

def process_chunk(chunk, data, radius=0.3):
    result = []
    # tqdm을 사용하여 진행률을 표시
    for idx, row in tqdm(chunk.iterrows(), total=chunk.shape[0], desc="Processing chunk"):
        count = 0
        cafe_location = (row['Latitude'], row['Longitude'])
        for other_idx, other_row in data.iterrows():
            if idx != other_idx:
                other_location = (other_row['Latitude'], other_row['Longitude'])
                distance = haversine(cafe_location, other_location)
                if distance <= radius:
                    count += 1
        result.append(count)
    return result

# 1번째 청크를 처리하고 결과 저장
result_chunk_1 = process_chunk(chunks[0], datanew)

result_chunk_1

# tqdm을 사용하여 진행률 표시
tqdm.pandas()

# 300m 반경 계산 함수
def count_cafes_in_range(row, data, radius=0.3):
    count = 0
    cafe_location = (row['Latitude'], row['Longitude'])  # 현재 카페의 좌표
    for idx, other_row in data.iterrows():
        if row.name != idx:  # 자기 자신을 제외한 다른 카페들에 대해 계산
            other_location = (other_row['Latitude'], other_row['Longitude'])
            distance = haversine(cafe_location, other_location)
            if distance <= radius:  # 300m 반경 내에 있는 카페인지 확인
                count += 1
    return count

# 카페 반경 내 카페 수를 계산해 새로운 컬럼 생성
# tqdm을 적용하여 진행률을 표시
datanew['cafeinrange'] = datanew.progress_apply(lambda row: count_cafes_in_range(row, datanew), axis=1)

# 결과 확인
print(datanew[['cafe_nm', 'cafeinrange']])

"""## 근방 쓰레기통 개수 세기"""

# 300m 반경 쓰레기통 개수 계산 함수
def count_trashbins_in_range(row, trashbin_data, radius=0.3):
    count = 0
    cafe_location = (row['Latitude'], row['Longitude'])  # 현재 카페의 좌표
    for idx, trash_row in trashbin_data.iterrows():
        if pd.notna(trash_row['Latitude']) and pd.notna(trash_row['Longitude']):  # 결측치 처리
            trash_location = (trash_row['Latitude'], trash_row['Longitude'])
            distance = haversine(cafe_location, trash_location)
            if distance <= radius:  # 300m 반경 내에 있는 쓰레기통인지 확인
                count += 1
    return count

# datanew와 trashbin 데이터프레임에서 쓰레기통 개수를 세어 새로운 컬럼 생성
datanew['trashbins_in_range'] = datanew.apply(lambda row: count_trashbins_in_range(row, trashbin), axis=1)

# 결과 확인
print(datanew[['cafe_nm', 'trashbins_in_range']])

"""## CPU 병렬처리"""

import pandas as pd
from haversine import haversine
from multiprocessing import Pool
from tqdm import tqdm
import numpy as np
import os

# 경로 설정 (저장하고자 하는 경로)
save_path = "/content/drive/MyDrive/0.DAB"

# 경로가 없으면 생성
if not os.path.exists(save_path):
    os.makedirs(save_path)

# 각 청크를 처리한 후, 해당 청크의 결과를 지정된 경로에 CSV 파일로 저장
def process_chunk(chunk, chunk_id, data, radius=0.3):
    result = []
    for idx, row in tqdm(chunk.iterrows(), total=chunk.shape[0], desc=f"Processing chunk {chunk_id}"):
        count = 0
        cafe_location = (row['Latitude'], row['Longitude'])
        for other_idx, other_row in data.iterrows():
            if idx != other_idx:
                other_location = (other_row['Latitude'], other_row['Longitude'])
                distance = haversine(cafe_location, other_location)
                if distance <= radius:
                    count += 1
        result.append(count)

    # 처리된 결과를 데이터프레임으로 변환하여 지정된 경로에 저장
    chunk['cafeinrange'] = result
    chunk_filename = os.path.join(save_path, f'chunk_{chunk_id}_result.csv')
    chunk.to_csv(chunk_filename, index=False)
    print(f"Chunk {chunk_id} saved to {chunk_filename}")
    return chunk_filename

# 병렬처리를 위한 함수 래퍼 (data를 전달하기 위한 트릭)
def process_chunk_wrapper(args):
    return process_chunk(*args)

# 전체 데이터 청크로 나누기 (예: 10개 청크로 나눔)
num_chunks = 10
chunks = np.array_split(datanew, num_chunks)

# 병렬처리를 사용하여 각 청크를 동시에 처리
if __name__ == '__main__':
    with Pool() as pool:
        saved_files = pool.map(process_chunk_wrapper, [(chunk, i+1, datanew) for i, chunk in enumerate(chunks)])

# 최종 파일을 하나로 병합할 수도 있음 (선택 사항)
merged_df = pd.concat([pd.read_csv(file) for file in saved_files])
merged_filename = os.path.join(save_path, 'merged_result.csv')
merged_df.to_csv(merged_filename, index=False)
print(f"All chunks have been processed and merged into '{merged_filename}'.")